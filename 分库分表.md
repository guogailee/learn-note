## 分库分表

```
分库
分表
```

## 为什么需要分库分表

```
1.QPS达到2000，数据库扛不住了

2.单表数据量太大，sql越来越慢
```

## 分库分表的好处

```
1.支持更高的并发：单库2000/s，多库2000*N/s

2.sql执行时间变短

3.数据库磁盘使用率保持健康状态
```

## 分库分表中间件

### proxy类

```
独立部署：server->proxy中间件->db

举例：
mycat 
```

### client类

```
 jar包：server使用client的api->db
 
 举例：
 sharding-jdbc
```

### sharding-jdb

- 特性

  ```
  支持分库分表、读写分离、分布式id生成、TCC事务等
  ```

- 缺点

  ```
  各个系统需要偶尔jar包的依赖
  ```

### mycat

- 特性

  ```
  需要部署，升级只需要中间件去升级就好
  ```

- 缺点

## 表垂直拆分

```
大表拆为多个小表：
主表放经常访问的字段
```

## 表水平拆分

```
1.把大表数据拆分到多个一样的表中，表结构一样

2.同库水平拆分 不同库水平拆分
常见的根据orderId取模定位库，再定位表
```

### 按照range拆分

```
好处：扩容方便，只需要准备新的库就可以

缺点：当月容易高流量
```

### 按照hash拆分

```
好处：平均每个库的压力

缺点：扩容麻烦
```

## 如何不停机迁移到分库分表

### 停机分库分表

```
集群部署每台机器开20个线程做数据迁移。

缺点：
停机几个小时
```

### 不停机双写方案

```
同时写旧库和新库，后台程序，把旧库数据导入新库，根据date_update，新值覆盖旧值，反复直到旧库数据全部导入新库。
```

## 数据库秒级平滑扩容架构方案



## 如何动态扩容缩容

```
一开始上来就是32个库，每个库32个表，1024张表，最多可以扩展到1024个数据库服务器，每个数据库服务器上面一个库一个表。

每个库正常承载的写入并发量是1000，那么32个库就可以承载32 * 1000 = 32000的写并发

路由配置：
库：orderId%32
表：orderId/32%32
```

## 分布式ID如何生成

### 数据库自增id方案

```
优点：方便简单

缺点：单库生成自增id，要是高并发的话，就会有瓶颈
```

### UUID

```
优点：就是本地生成，不要基于数据库

缺点：
UUID太长了，作为主键性能太差，不适合用于主键

UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作

适用场景：
如果你是要随机生成个什么文件名，编号之类的，你可以用UUID，但是作为主键是不能用UUID
```

### snowflake算法

```
64位的long型的id：
1 bit：不用，因为二进制里第一个bit为如果是1，那么都是负数，但是我们生成的id都是正数，所以第一个bit统一都是0

41 bit：表示的是时间戳，单位是ms

10 bit：记录工作机器id，5个bit代表机房id，5个bit代表机器id

12 bit：记录同一个毫秒内产生的不同id
2 ^ 12 - 1 = 4096

该算法可以确保每个机房每个机器每一毫秒，最多生成4096个不重复的id
```

## 为何需要读写分离

```
缓存数据还没导入，或者内存满了，删除了一些，会走数据库。

假设一台mysql的写请求1000，读请求1000，就会有压力，所以需要读写分离，缓存没数据的时候，从从库来读，减压到1000

一主多从，一般3-5台
```

## 主从复制原理

```
主库将变更写binlog日志，然后从库连接到主库后，从库有一个I/O线程，将主库的binlog日志拷贝到本地，写入一个中继日志

接着从库中有一个SQL线程会从中继日志读取binlog，然后执行binlog日志中的内容

即在本地再次执行一遍SQL，确保跟主库的数据相同
```

## 主从延迟如何解决

### 半同步复制(semi-sync)-解决主库数据丢失问题

```
主库写入binlog日志后，就会强制此时立即将数据同步到从库

从库将日志写入自己本地的relay log后，会返回一个ack给主库

主库接收到至少一个从库的ack之后才会认为写操作完成

优点：利用数据库原生功能，比较简单

缺点：主库的写请求时延会增长，吞吐量会降低
```

### 数据库中间件

```
1）所有的读写都走数据库中间件，通常情况下，写请求路由到主库，读请求路由到从库

2）记录所有路由到写库的key，在主从同步时间窗口内（假设是500ms），如果有读请求访问中间件，此时有可能从库还是旧数据，就把这个key上的读请求路由到主库。

3）在主从同步时间过完后，对应key的读请求继续路由到从库。

数据库中间件：canal

优点：能保证绝对一致

缺点：数据库中间件的成本较高
```

### 缓存记录写key法

```
写流程：

1）如果key要发生写操作，记录在cache里，并设置“经验主从同步时间”的cache超时时间，例如500ms

2）然后修改主数据库

读流程：

1）先到缓存里查看，对应key有没有相关数据

2）有相关数据，说明缓存命中，这个key刚发生过写操作，此时需要将请求路由到主库读最新的数据。

3）如果缓存没有命中，说明这个key上近期没有发生过写操作，此时将请求路由到从库，继续读写分离。

优点：相对数据库中间件，成本较低

缺点：
为了保证“一致性”，引入了一个cache组件，并且读写数据库时都多了缓存操作。
```

## 数据库和缓存一致性方案

### 先更新数据库，再删缓存

```
该方案下脏数据场景：
1.缓存刚好失效
2.A查询数据库，得到一个旧值
3.此时B将新值写入数据库
4.请求B删除缓存
5.请求A将查询的旧值写入缓存

ok，如果发送上述情况，确实是会发生脏数据；前提条件是A的读请求比B的写请求慢，一般不可能，如果一定要预防这种情况呢？

延迟双删：更新完以后，sleep一段时间，再删除缓存一次。
睡眠时间修改为在主从同步的延时时间基础上，加几百ms。
public void write(String key,Object data){
        db.updateData(data);
        redis.delKey(key);
        Thread.sleep(1000);
        redis.delKey(key);
    }

如果延迟双删，第二次删除失败如何解决？
提供重试保障机制-订阅binlog，重试
```

